{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Giving a shit about statistics\n",
    "\n",
    "Every time I take a statistics course -- one that's about inference, design, etc. and less about math, algorithms, and such -- I really have a hard time giving a shit. It feels so boring. I make this document in hopes of giving some shits, but I am doubtful. Nonetheless, I try. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Julia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Recompiling stale cache file /home/mosiman/.julia/compiled/v1.1/Plots/ld3vC.ji for Plots [91a5bcdd-55d7-5caf-9e0b-520d859cae80]\n",
      "└ @ Base loading.jl:1184\n",
      "┌ Info: Recompiling stale cache file /home/mosiman/.julia/compiled/v1.1/StatsPlots/SiylL.ji for StatsPlots [f3b207a7-027a-5e70-b257-86293d7955fd]\n",
      "└ @ Base loading.jl:1184\n"
     ]
    }
   ],
   "source": [
    "using Plots\n",
    "using Statistics\n",
    "using LinearAlgebra\n",
    "using StatsPlots\n",
    "using StatsBase\n",
    "using Distributions\n",
    "using HypothesisTests\n",
    "using DataFrames\n",
    "using CSV\n",
    "using Optim\n",
    "using GLM\n",
    "using LaTeXStrings\n",
    "using CategoricalArrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### London bikeshare data\n",
    "\n",
    "from kaggle! https://www.kaggle.com/hmavrodiev/london-bike-sharing-dataset/data\n",
    "\n",
    "```\n",
    "\"timestamp\" - timestamp field for grouping the data\n",
    "\"cnt\" - the count of a new bike shares\n",
    "\"t1\" - real temperature in C\n",
    "\"t2\" - temperature in C \"feels like\"\n",
    "\"hum\" - humidity in percentage\n",
    "\"wind_speed\" - wind speed in km/h\n",
    "\"weather_code\" - category of the weather\n",
    "\"is_holiday\" - boolean field - 1 holiday / 0 non holiday\n",
    "\"is_weekend\" - boolean field - 1 if the day is weekend\n",
    "\"season\" - category field meteorological seasons: 0-spring ; 1-summer; 2-fall; 3-winter.\n",
    "\n",
    "\"weathe_code\" category description:\n",
    "1 = Clear ; mostly clear but have some values with haze/fog/patches of fog/ fog in vicinity\n",
    "2 = scattered clouds / few clouds\n",
    "3 = Broken clouds\n",
    "4 = Cloudy\n",
    "7 = Rain/ light Rain shower/ Light rain\n",
    "10 = rain with thunderstorm\n",
    "26 = snowfall\n",
    "94 = Freezing Fog\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: `head(df::AbstractDataFrame)` is deprecated, use `first(df, 6)` instead.\n",
      "│   caller = top-level scope at In[2]:2\n",
      "└ @ Core In[2]:2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>timestamp</th><th>cnt</th><th>t1</th><th>t2</th><th>hum</th><th>wind_speed</th><th>weather_code</th></tr><tr><th></th><th>String</th><th>Int64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th></tr></thead><tbody><p>6 rows × 10 columns (omitted printing of 3 columns)</p><tr><th>1</th><td>2015-01-04 00:00:00</td><td>182</td><td>3.0</td><td>2.0</td><td>93.0</td><td>6.0</td><td>3.0</td></tr><tr><th>2</th><td>2015-01-04 01:00:00</td><td>138</td><td>3.0</td><td>2.5</td><td>93.0</td><td>5.0</td><td>1.0</td></tr><tr><th>3</th><td>2015-01-04 02:00:00</td><td>134</td><td>2.5</td><td>2.5</td><td>96.5</td><td>0.0</td><td>1.0</td></tr><tr><th>4</th><td>2015-01-04 03:00:00</td><td>72</td><td>2.0</td><td>2.0</td><td>100.0</td><td>0.0</td><td>1.0</td></tr><tr><th>5</th><td>2015-01-04 04:00:00</td><td>47</td><td>2.0</td><td>0.0</td><td>93.0</td><td>6.5</td><td>1.0</td></tr><tr><th>6</th><td>2015-01-04 05:00:00</td><td>46</td><td>2.0</td><td>2.0</td><td>93.0</td><td>4.0</td><td>1.0</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccccc}\n",
       "\t& timestamp & cnt & t1 & t2 & hum & wind\\_speed & weather\\_code & \\\\\n",
       "\t\\hline\n",
       "\t& String & Int64 & Float64 & Float64 & Float64 & Float64 & Float64 & \\\\\n",
       "\t\\hline\n",
       "\t1 & 2015-01-04 00:00:00 & 182 & 3.0 & 2.0 & 93.0 & 6.0 & 3.0 & $\\dots$ \\\\\n",
       "\t2 & 2015-01-04 01:00:00 & 138 & 3.0 & 2.5 & 93.0 & 5.0 & 1.0 & $\\dots$ \\\\\n",
       "\t3 & 2015-01-04 02:00:00 & 134 & 2.5 & 2.5 & 96.5 & 0.0 & 1.0 & $\\dots$ \\\\\n",
       "\t4 & 2015-01-04 03:00:00 & 72 & 2.0 & 2.0 & 100.0 & 0.0 & 1.0 & $\\dots$ \\\\\n",
       "\t5 & 2015-01-04 04:00:00 & 47 & 2.0 & 0.0 & 93.0 & 6.5 & 1.0 & $\\dots$ \\\\\n",
       "\t6 & 2015-01-04 05:00:00 & 46 & 2.0 & 2.0 & 93.0 & 4.0 & 1.0 & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "6×10 DataFrame. Omitted printing of 4 columns\n",
       "│ Row │ timestamp           │ cnt   │ t1      │ t2      │ hum     │ wind_speed │\n",
       "│     │ \u001b[90mString\u001b[39m              │ \u001b[90mInt64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m    │\n",
       "├─────┼─────────────────────┼───────┼─────────┼─────────┼─────────┼────────────┤\n",
       "│ 1   │ 2015-01-04 00:00:00 │ 182   │ 3.0     │ 2.0     │ 93.0    │ 6.0        │\n",
       "│ 2   │ 2015-01-04 01:00:00 │ 138   │ 3.0     │ 2.5     │ 93.0    │ 5.0        │\n",
       "│ 3   │ 2015-01-04 02:00:00 │ 134   │ 2.5     │ 2.5     │ 96.5    │ 0.0        │\n",
       "│ 4   │ 2015-01-04 03:00:00 │ 72    │ 2.0     │ 2.0     │ 100.0   │ 0.0        │\n",
       "│ 5   │ 2015-01-04 04:00:00 │ 47    │ 2.0     │ 0.0     │ 93.0    │ 6.5        │\n",
       "│ 6   │ 2015-01-04 05:00:00 │ 46    │ 2.0     │ 2.0     │ 93.0    │ 4.0        │"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bikedata = CSV.read(\"london_merged.csv\")\n",
    "head(bikedata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: `T` is deprecated, use `nonmissingtype` instead.\n",
      "│   caller = catvaluetype at array.jl:614 [inlined]\n",
      "└ @ Core /home/mosiman/.julia/packages/CategoricalArrays/xjesC/src/array.jl:614\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "17414-element CategoricalArray{Float64,1,UInt32}:\n",
       " 3.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 4.0\n",
       " 4.0\n",
       " 4.0\n",
       " 3.0\n",
       " 3.0\n",
       " 3.0\n",
       " 4.0\n",
       " ⋮  \n",
       " 3.0\n",
       " 4.0\n",
       " 3.0\n",
       " 4.0\n",
       " 4.0\n",
       " 3.0\n",
       " 2.0\n",
       " 3.0\n",
       " 4.0\n",
       " 4.0\n",
       " 4.0\n",
       " 2.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: `T` is deprecated, use `nonmissingtype` instead.\n",
      "│   caller = show(::IOContext{Base.GenericIOBuffer{Array{UInt8,1}}}, ::CategoricalValue{Float64,UInt32}) at value.jl:104\n",
      "└ @ CategoricalArrays /home/mosiman/.julia/packages/CategoricalArrays/xjesC/src/value.jl:104\n",
      "┌ Warning: `T` is deprecated, use `nonmissingtype` instead.\n",
      "│   caller = show(::IOContext{Base.GenericIOBuffer{Array{UInt8,1}}}, ::CategoricalValue{Float64,UInt32}) at value.jl:104\n",
      "└ @ CategoricalArrays /home/mosiman/.julia/packages/CategoricalArrays/xjesC/src/value.jl:104\n"
     ]
    }
   ],
   "source": [
    "# make weather_code categorical\n",
    "bikedata.weather_code = categorical(bikedata.weather_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# realized I should probably have done this before all the analysis, so adding it to the top here\n",
    "# make timestamp a datetime\n",
    "\n",
    "using Dates\n",
    "\n",
    "dateformat = DateFormat(\"Y-m-d H:M:S\")\n",
    "\n",
    "bikedata.timestamp = DateTime.(bikedata.timestamp, dateformat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BIKENUMSAMPLE = 250\n",
    "bike = bikedata[sample(axes(bikedata, 1), BIKENUMSAMPLE; replace = false, ordered = true), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_filter = filter(x -> x.cnt > 0, bike)\n",
    "scatter(bike_filter.t1, bike_filter.cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model we use to describe linearly correlated data is \n",
    "\n",
    "$$\n",
    "y = \\beta_0 x + \\beta_1 + \\epsilon\n",
    "$$\n",
    "\n",
    "Which is quite reminiscent of our highschool $y = mx + b$ days, except we have a random term $\\epsilon$ where $\\epsilon \\sim N(0,\\sigma^2)$. \n",
    "\n",
    "We assume, for simplicity, that our $x$ is 'controllable', that is, not random. This gives us our first important result, which is that our $y$ are normally distributed with the same variance as our random terms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "y_i \\sim N(\\beta_0 + \\beta_1 x_i, \\sigma^2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we find out what the parameters are? We have the following two approaches:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Least Squares Estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The least squares estimates are exactly what they sound like. They are the values that minimize the squared distance between the observed response and a fitted line. I.e.,\n",
    "\n",
    "$$\n",
    "\\arg \\min \\sum_{i=1}^n \\left[ y_i - (\\hat\\beta_0 + \\hat\\beta_1 x_i) \\right]^2\n",
    "$$\n",
    "\n",
    "Where the hats represent the fact that these are only _estimates_ for the true parameter values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The minimum values can be found analytically in this simple case. Using some partial derivatives and finding extreme values (also, checking the second derivatives), it is not too difficult to come to the conclusion:\n",
    "\n",
    "$$\n",
    "\\hat\\beta_0 = \\bar y - \\hat\\beta_1 \\bar x\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat\\beta_1 = \\frac{\\sum x_i y_i - n \\bar x \\bar y}{\\sum x_i^2 - n\\bar x^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this to fit a model for temperature vs number of bikes ridden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = size(bike,1)\n",
    "x̄ = mean(bike.t1)\n",
    "ȳ = mean(bike.cnt)\n",
    "β₁ = (sum([ b.t1 * b.cnt for b in eachrow(bike)]) - n*x̄*ȳ ) / ( sum([b.t1^2 for b in eachrow(bike)]) - n*x̄^2)\n",
    "β₀ = ȳ - β₁ * x̄\n",
    "\n",
    "(β₀, β₁)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(bike.t1, bike.cnt, xlab=\"temperature\", ylab=\"count\", label=\"Bike count\")\n",
    "plot!( x -> β₀ + β₁*x, 0:30, linewidth=3, label=\"Fitted regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The least squares estimates also provide us with some properties of residuals. Note that because the least squares estimates satisfy \n",
    "\n",
    "$$\n",
    "(\\hat\\beta_0, \\hat\\beta_1) = \\arg \\min \\sum_{i=1}^n (\\hat y_i - y_i)^2 \n",
    "$$\n",
    "\n",
    "We have the following conditions as a consequence of $\\hat\\beta_0$ and $\\hat\\beta_1$ being critical values\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    0 & = \\sum_{i=1}^n (y_i - \\hat\\beta_0 - \\hat\\beta_1x_i) \\\\\n",
    "        & = \\sum_{i=1}^n r_i = 0 \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    0 & = \\sum_{i=1}^n (y_i - \\hat\\beta_0 - \\hat\\beta_1x_i) (-x_i)\\\\\n",
    "        & = \\sum_{i=1}^n (y_i - \\hat y_i)(-x_i) \\\\\n",
    "        & = \\sum_{i=1}^n \\hat y_i x_i - y_i x_i \\\\\n",
    "        & = \\sum_{i=1}^n (\\hat y_i - y_i)x_i \\\\\n",
    "        & = \\sum_{i=1}^n r_i x_i\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Using the fact that these sums involving residuals go to $0$ using least squares estimates, we can conclude one more: $\\sum_{i=1}^n r_i \\hat y_i = 0$. \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\sum r_i \\hat y_i = 0 & \\iff \\sum r_i x_i + \\sum r_i \\hat y_i = 0\\\\\n",
    "                                            & \\iff \\sum r_i (\\hat y_i + x_i) = 0 \\\\\n",
    "                                            & \\iff ( \\hat\\beta_0 \\sum r_i) + (\\hat\\beta_1 \\sum r_i x_i ) + (\\sum r_i x_i)  = 0\\\\\n",
    "                                            & \\iff 0 + 0 + 0 = 0\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maximum likelihood estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum likelihood estimates come from the following:\n",
    "\n",
    "$$\n",
    "\\arg \\max L(\\theta | x_1, \\cdots, x_n)\n",
    "$$\n",
    "\n",
    "where  $L(\\theta | x_1, \\cdots, x_n) = \\Pi_{i=1}^n f(x_i | \\theta)$. \n",
    "\n",
    "Note that in our case, we would like to estimate the $\\beta_0, \\beta_1$ parameters. The probability function for $y$ is normal, in fact, $y_i \\sim N(\\beta_0 + \\beta_1 x_i, \\sigma^2)$. Therefore, we can solve for the maximum likelihood of three parameters: $\\beta_0, \\beta_1, \\sigma$. \n",
    "\n",
    "Use the log likelihood, as $\\log$ is a strictly increasing function and therefore preserves our critical values.\n",
    "\n",
    "The values we find are\n",
    "\n",
    "- $\\hat\\beta_0 = \\bar y - \\beta_1 \\bar x$\n",
    "- $\\hat\\beta_1 = \\frac{\\sum x_i y_i - n \\bar x \\bar y}{\\sum x_i^2 - n\\bar x^2}$\n",
    "- $\\hat\\sigma^2 = \\frac 1n \\sum_{i=1}^n r_i^2$\n",
    "\n",
    "Note: that maximum likelihood estimate gives a __biased__ estimator for $\\sigma^2$, but our estimates for $\\hat\\beta_0$ and $\\hat\\beta_1$ are identical to the least squares estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numerical estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can estimate numerically these quantities with some sort of numerical solver.  We'll use Julia's `Optim.jl` package. \n",
    "\n",
    "this was a bust. Redo when possible!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the pdf of the response, since the response is normal\n",
    "# θ = [β₀, β₁, σ]\n",
    "f(x, θ) = pdf(Normal(θ[1] + θ[2]*x, θ[3]^2), x)\n",
    "L(θ) = prod(map( x -> f(x, θ), bike.t1))\n",
    "l(θ) = sum(map(x -> log(f(x, θ)), bike.t1) )\n",
    "\n",
    "optimize(x -> l(x), [β₀, β₁, 100000.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypothesis testing is to inquire about the validity of a statement about a parameter. I have written a little on the logic behind this. The gist is, we need distributional results on our parameters. This is not a crazy question: our estimates are a function of the response, which we have concluded are random variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $t$ statistic and tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A $t$ statistic can be thought of as a $z$ statistic, but having replaced the unknown quantity $\\sigma^2$ with the estimate $S^2$.  That is\n",
    "\n",
    "$$\n",
    "t = \\frac{\\bar X - \\mu }{S / \\sqrt{n} } \\sim t_{n-1}\n",
    "$$\n",
    "\n",
    "The $t$ statistic follows a $t$ distribution with $n-1$ degrees of freedom. \n",
    "\n",
    "By assuming the null hypothesis, the $t$ statistic can be computed. The probability distribution of the $t$ statistic allows us to conclude if the probability of observing something as unlikely as our statistic is probably, thus garning evidence for the rejection of lack thereof of the hypothesis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $f$ statistic and tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The usual characterization for an $F$ statistic is that a quantity $F$ has an $F$ distribution if it assumes the form \n",
    "\n",
    "$$\n",
    "F = \\frac{U_1 / d_1}{U_2 / d_2} \\sim F_{d_1, d_2}\n",
    "$$\n",
    "\n",
    "I.e., if $F$ can be expressed as the ratio of two quantities $U_1$ and $U_2$ who are $\\chi^2$ distributed with $d_1$ and $d_2$ degrees of freedom, respectively, then $F$ is $F_{d_1, d_2}$-distributed.\n",
    "\n",
    "Below, we see that SSE and SSR are both $\\chi^2$ distributed. In fact, they have degrees of freedom $1, n-2$ respectively. Thus an $F$ test arising naturally from the data has the form \n",
    "\n",
    "$$\n",
    "F = \\frac{\\text{SSR}/1}{\\text{SSE}/(n-2)} = \\frac{\\text{MSR}}{\\text{MSE}} \\sim F_{1, n-2}\n",
    "$$\n",
    "\n",
    "SSR encodes the descrepency between a basic bitch model $y = b$ and a big brain model $y = mx + b$. If SSR is large, i.e., the ratio $\\frac{\\text{MSR}}{\\text{MSE}}$ is large, then we likely have data dependent on $x_i$'s. \n",
    "\n",
    "This is more or less equivalent to testing if $\\beta_1 = 0$. In fact, $\\text{MSR} / \\text{MSE}$ is $F$-distributed only under the null hypothesis that $\\beta_1 = 0$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(x -> pdf(FDist(1, 3), x), 0:0.003:2.5, label=L\"F_{1, 3}\", linewidth=3, title=\"F distribution with 1,3 d.f.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANOVA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't understand which it's such a big deal.. and I haven't heard this term ever used in a natural way. Whatever. Supposedly, any decent exposition on this subject starts with the topic of decomposing variance. \n",
    "\n",
    "Recall the variance \n",
    "\n",
    "$$\n",
    "\\text{var}(y) = \\sum (y_i - \\mu)^2\n",
    "$$\n",
    "\n",
    "I.e., the sum squared distance from the mean. We don't have the mean, so consider instead\n",
    "\n",
    "$$\n",
    "\\sum (y_i - \\bar y )^2\n",
    "$$\n",
    "\n",
    "A decomposition:\n",
    "\n",
    "$$\n",
    "\\sum (y_i - \\bar y)^2 = \\left[ \\sum (y_i - \\hat y_i)^2 \\right] + \\left[ \\sum (\\hat y_i - \\bar y)^2 \\right]\n",
    "$$\n",
    "\n",
    "Or:\n",
    "\n",
    "$$\n",
    "\\text{SST} = \\text{SSE} + \\text{SSR}\n",
    "$$\n",
    "\n",
    "Consider the following proof by picture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [ 1 , 3 , 4 ,10 ,11 ,15 ,16 ,17]\n",
    "ys = [21.55219363265408  ,  8.70482469035426  , 27.451668121513542 , 43.66593098495369  , 46.84274178061619  , 77.08568993110022  , 99.3367189562757   , 90.41233511277945  ]\n",
    "\n",
    "df = DataFrame(X = xs, Y = ys)\n",
    "lmfit = lm(@formula(Y ~ X), df)\n",
    "\n",
    "β₀, β₁= coef(lmfit)\n",
    "\n",
    "ȳ = mean(ys)\n",
    "\n",
    "i = 7\n",
    "ŷᵢ_pos = [xs[7], β₀ + β₁*xs[7]]\n",
    "ȳ_pos = [xs[7], ȳ]\n",
    "yᵢ_pos = [xs[7], ys[7]]\n",
    "\n",
    "scatter(xs, ys, label= \"\", legend=:left, title=\"Visual proof of total variance decomposition\")\n",
    "plot!(x -> β₀ + β₁*x, 0:20, label= L\"\\hat y_i\")\n",
    "plot!(x -> ȳ, 0:20, label=L\"\\bar y\")\n",
    "#plot!(yᵢ_pos, ŷᵢ_pos)\n",
    "plot!([xs[7], xs[7]], [β₀ + β₁*xs[7], ys[7]], label=L\"\\hat y_i - y_i\", linewidth=2)\n",
    "plot!([xs[7], xs[7]], [β₀ + β₁*xs[7], ȳ], label=L\"\\hat y_i - \\bar y\", linewidth=2)\n",
    "plot!([xs[6]+0.5, xs[6]+0.5], [ȳ, ys[7]], label=L\"y_i - \\bar y\", linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, pointwise, the distance between $y_i$ and the mean $\\bar y$ is clearly the distance from $y_i$ to $\\hat y_i$ plus the distance from $\\hat y_i$ to $\\bar y$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To $\\mathbb{R}^2$ and beyond "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's model this shit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cornerplot(hcat(bike.cnt, bike.t1, bike.t1, bike.hum, bike.wind_speed, bike.weather_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@df bike corrplot(cols(2:6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like `t1`, `t2`, `hum`, and `wind_speed` are pretty skewed. I wouldn't particularly expect count to be 'normal', more like poisson so thats good. \n",
    "\n",
    "Count is somewhat linearly correlated with temperature, negatively correlated with humidity. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try a model with t1, t2, hum, wind_speed\n",
    "\n",
    "bikeBigModel = lm(@formula(cnt ~ t1 + t2 + hum + wind_speed + weather_code), bike)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I include the entire dataset, I get more significant results. I'll try that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikeBigModel = lm(@formula(cnt ~ t1 + t2 + hum + wind_speed + weather_code), bikedata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So temperature, humidity, wind speed, are significant, and certain weather codes (2 (cloudy), 3 (broken clouds), maybe even 4 ) are more significant compared to weather code 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function vif(dat)\n",
    "    # VIF_j = 1 / (1 - Rⱼ²)\n",
    "    # for each variable $xⱼ$, regress $xⱼ$ on all other variables \n",
    "    # assume dat's variables can be accessed by indexing, i.e., dat[j]\n",
    "    # also assume it has a header, so basically dat should be a dataframe.\n",
    "    vars = names(dat)\n",
    "    r2_vals = []\n",
    "    for j in 1:size(dat)[2]\n",
    "        # regress x_j on all others\n",
    "        fit_j = lm(Matrix(dat[:,1:end .!= j]), dat[j])\n",
    "        append!(r2_vals, r2(fit_j))\n",
    "    end\n",
    "    vif_vals = map(x -> 1 / (1-x), r2_vals)\n",
    "    return vif_vals\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif(bike[[:cnt, :t1, :t2, :hum, :wind_speed]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that `t1` and `t2` are very collinear, so let's just take `t1`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikeModel = lm(@formula(cnt ~ t1 + hum + wind_speed + weather_code), bikedata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get some diagnostic plots going"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nevermind, wow lol it's so easy in R but there are no Julia functions for easy diagnostic plots from a dataframe / linear model rip\n",
    "\n",
    "From above it looked like humidity and wind speed were skewed, so we can try a log or power transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# box cox \n",
    "using BoxCoxTrans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "an example of optimizing for the appropriate $\\lambda$ for the power transform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BoxCoxTrans.lambda(bike.wind_speed, interval=(-5,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a problem, because the box cox transform requires that $y_i > 0$ strictly, some of the wind speed is $0$ for some days.\n",
    "\n",
    "Out of $17414$ days there are 68 days with \"0\" wind speed. I'm not sure if this is N/A or if there was actually just no wind.\n",
    "\n",
    "Some googling suggests its not uncommon for calm days (less than 1kph) to be marked as 0 wind speed. In that case, we should just add a small epsilon to any 0 wind speed days. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wind_speed_plusEpsilon = map(x -> iszero(x) ? x += 0.1 : x, bikedata.wind_speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "λhum = BoxCoxTrans.lambda(bikedata.hum, interval=(-5,5)).value\n",
    "λwind_speed = BoxCoxTrans.lambda(wind_speed_plusEpsilon, interval=(-5,5)).value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's transform them and make a new dataframe with the appropriate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikedata_pt = copy(bikedata)\n",
    "\n",
    "bikedata_pt.wind_speed = BoxCoxTrans.transform(wind_speed_plusEpsilon, λwind_speed)\n",
    "bikedata_pt.hum = BoxCoxTrans.transform(bikedata.hum, λhum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikeModelPT = lm(@formula(cnt ~ t1 + hum + wind_speed + weather_code), bikedata_pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting all the bike data takes way too long on my potato laptop. I tried using a 500 element subset, but it was too small and missed out on many things. I will try a 2500 element subset just for plotting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_pt = bikedata_pt[sample(axes(bikedata_pt, 1), 2500; replace = false, ordered = true), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@df bike_pt corrplot(cols(2:6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, are any of the weather codes significant? We only know that weather codes 2, 3, and 4 are significant but only compared to weather code 1. We can use an F test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitWithoutWeatherCode = lm(@formula(cnt ~ t1 + hum + wind_speed), bikedata_pt)\n",
    "fitWithWeatherCode = lm(@formula(cnt ~ t1 + hum + wind_speed + weather_code), bikedata_pt)\n",
    "\n",
    "ftest(fitWithWeatherCode.model, fitWithoutWeatherCode.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is significant, by a lot, so let's try this as a final model.\n",
    "\n",
    "We can try plotting the true data versus our model's predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll take a subset of the data, but first we should convert the timestamp column to an actual DateTime format (instead of a plain string) so we can query it easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#plotregion = bikedata[ \n",
    " #   (Dates.month(bikedata_pt.timestamp) == 8) .& (Dates.year(bikedata_pt.timestamp) == 2016)\n",
    "#]\n",
    "\n",
    "#plotregion = map(x -> Dates.month(x) == 8 && Dates.year(x) == 2016 ? 1 : 0, \n",
    "#    bikedata_pt.timestamp)\n",
    "plotregion = filter(x-> Dates.week(x) == 33 && Dates.year(x) == 2016,\n",
    "    bikedata_pt.timestamp)\n",
    "\n",
    "df_subset = filter(row -> row.timestamp in plotregion, bikedata_pt)\n",
    "\n",
    "# predicted values in plotregion\n",
    "\n",
    "predicted = predict(bikeModelPT, df_subset )\n",
    "\n",
    "plot(plotregion, df_subset.cnt)\n",
    "plot!(plotregion, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wow, that's pretty shit lol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "looks like the predicted values peak at noon-ish, and I think that's because that's when temperature would be highest. \n",
    "\n",
    "In the actual data, we have a sort of bimodal distribution over the day, which I think might map to going to work and coming back from work. There's a clear lull in the middle of the day. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What I had in mind for my model, was that it could be used to take a weekly forecast and appropriately schedule workers for that day / week. For example, I know in Toronto the bikeshare system requires workers to load balance the access points, so there's always some bikes at each (and space at each, to park). By predicting usage, we may also be able to predict how many people we'll need to shuffle around bicycles. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the same model, but change the dataset a bit. Instead of hourly counts, let's take daily counts. We'll then use variables `t_hi` and `t_lo` to represent the high and lows of that day. Let's also take average wind speed and the mode of the weather code for that day. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll load some more packages to help with the data wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikedaily = bikedata |> \n",
    "    @groupby(x -> Date(x.timestamp))  |> \n",
    "    @map( {Date = key(_), \n",
    "                    cnt = mean(_.cnt), \n",
    "                    thi = maximum(_.t1), \n",
    "                    tlo = minimum(_.t1),\n",
    "                    humhi = maximum(_.hum), \n",
    "                    humlo = minimum(_.hum),\n",
    "                    wind_speed = mean(_.wind_speed),\n",
    "                    weather_code=mode(_.weather_code)}) |> \n",
    "    DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cornerplot\n",
    "@df bikedaily corrplot(cols(2:7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting to note, is that `cnt` looks like a different poisson distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dailyfit = lm(@formula(cnt ~ thi + tlo + humhi + humlo + wind_speed + weather_code), bikedaily)\n",
    "dailyfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ah oops, I want the t-test to compare levels with weather code 1 as the 'reference' level. We reorder the categorical variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levels!(bikedaily.weather_code, [1.0, 2.0, 3.0, 4.0, 7.0, 26.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dailyfit = lm(@formula(cnt ~ thi + tlo + humhi + humlo + wind_speed + weather_code), bikedaily)\n",
    "dailyfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I know `thi` and `tlo` are collinear, so are `humhi` and `humlo`. We can see this using the variance inflation factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif(bikedaily[[:thi, :tlo, :humhi, :humlo, :wind_speed]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, not as collinear as I thought."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, it looks like the weather codes are not as significant as they were before. It's worth testing to see if weather codes are significant at all. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitWithoutWeatherCode = lm(@formula(cnt ~ thi + tlo + humhi + humlo + wind_speed), bikedaily)\n",
    "fitWithWeatherCode = lm(@formula(cnt ~ thi + tlo + humhi + humlo + wind_speed + weather_code), bikedaily)\n",
    "\n",
    "ftest(fitWithWeatherCode.model, fitWithoutWeatherCode.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weather codes are significant. So I guess we can conclude that the weather conditions do not really significantly affect ridership when compared to the reference level (clear skies), unless it is weather code 7 (rainy!).\n",
    "\n",
    "Because we took the mode of the weather codes for the day, it looks like we're missing out on some weather codes but that's fine I suppose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try comparing our model like above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plotregion = filter(x-> Dates.month(x) in [6,7,8,9] && Dates.year(x) == 2016,\n",
    "    bikedaily.Date)\n",
    "\n",
    "df_subset = filter(row -> row.Date in plotregion, bikedaily)\n",
    "\n",
    "# predicted values in plotregion\n",
    "\n",
    "predicted = predict(dailyfit, df_subset )\n",
    "\n",
    "plot(plotregion, df_subset.cnt, lw=2, label=\"Historical\")\n",
    "plot!(plotregion, predicted, lw=2, label=\"Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to be doing a lot better, although it feels suspiciously like overfitting, since all the movements are the very similar, only differing in magnitude. \n",
    "\n",
    "It would be nice to plot some confidence bands. Gotta pull out some math though, I think, since the packages won't do it for me"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall some common properties\n",
    "\n",
    "- $\\hat \\beta \\sim MVN(\\beta, \\sigma^2(X^TX)^{-1})$\n",
    "\n",
    "    Since $\\hat \\beta = Hy = (X^TX)^{-1}X^T(X\\beta + \\epsilon)$, everything is deterministic other than epsilon, which has distribution $N(0,\\sigma^2)$.  So we know that $\\hat \\beta$ is normal. We just need its mean and variance. \n",
    "    \n",
    "    The mean comes easily, since $E(\\epsilon) = 0$ and the $X$s cancel out. We end up with $E(\\hat \\beta) = \\beta$. Unbiased, nice.\n",
    "    \n",
    "    The variance is more algebra as well. \n",
    "    \n",
    "    $$\n",
    "    \\begin{aligned}\n",
    "        \\text{Var}(\\hat \\beta) & = \\text{Var}((X^TX)^{-1}X^T(X\\beta + \\epsilon)) \\\\\n",
    "                                               & = \\text{Var}(\\beta + (X^TX)^{-1}X^T\\epsilon) \\\\\n",
    "                                               & = 0 + \\text{Var}((X^TX)^{-1}X^T\\epsilon) \\\\\n",
    "                                               & = (X^TX)^{-1}X^T \\text{Var}(\\epsilon) ( (X^TX)^{-1}X^T)^T \\\\\n",
    "                                               & = \\sigma^2 (X^TX)^{-1}\n",
    "     \\end{aligned}\n",
    "     $$\n",
    "     \n",
    "- Prediction intervals \n",
    "\n",
    "    Consider the prediction error \n",
    "    \n",
    "    $$\n",
    "    y_p - \\hat \\mu_p\n",
    "    $$\n",
    "    \n",
    "    Where $y_p$ is the true value at $x_p$ and $\\hat \\mu_p$ is the predicted value at $x_p$. \n",
    "    \n",
    "    We can construct a distribution on the prediction error. Note that $y_p - \\mu_p = \\mu_p + \\epsilon - \\hat \\mu_p$. Therefore we know it's normally distributed. The mean is clearly zero, since $\\hat \\mu_p = x_p^T \\beta$ is unbiased. As for the variance,\n",
    "    \n",
    "    $$\n",
    "    \\begin{aligned}\n",
    "        \\text{Var}(y_p - \\hat \\mu_p) & = 0 + \\sigma^2 + \\text{Var}(\\hat \\mu_p) \\\\\n",
    "                                                          & = \\sigma^2 + \\text{Var}(x_p^T \\hat\\beta) \\\\\n",
    "                                                          & = \\sigma^2 + x_p^T \\sigma^2 I (X^TX)^{-1} x_p \\\\\n",
    "                                                          & = \\sigma^2 ( 1 + x_p^T (X^TX)^{-1} x_p )\n",
    "     \\end{aligned}\n",
    "     $$\n",
    "     \n",
    "     So $y_p - \\hat \\mu_p \\sim N(0, \\sigma^2(1 + x_p^T (X^TX)^{-1} x_p)$. \n",
    "     \n",
    "     Then we have \n",
    "     \n",
    "     $$\n",
    "     \\frac{(y_p - \\hat \\mu_p) - 0}{ \\text{se}(y_p - \\hat\\mu_p) } \\sim t_{n-p-1}\n",
    "     $$\n",
    "     \n",
    "     where $\\text{se}(y_p - \\hat\\mu_p) = \\sqrt{\\hat\\sigma^2(1 + x_p^T (X^TX)^{-1} x_p)}$\n",
    "     \n",
    "     Therefore, a confidence interval for the prediction with confidence level $\\alpha$ is \n",
    "     \n",
    "     $$\n",
    "     \\hat\\mu_p \\pm t_{n-p-1}^{\\alpha / 2} \\text{se}(y_p - \\hat\\mu_p)\n",
    "     $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yeah, I don't think this is the best (numerically), i.e. not taking advantage of matrix properties but whatever\n",
    "\n",
    "\n",
    "α = 0.05\n",
    "X = dailyfit.mm.m # design matrix\n",
    "e = residuals(dailyfit)\n",
    "S² = e' * e / (length(e) - 11)\n",
    "tᵅ = ccdf(TDist(length(e) - 11), α/2)\n",
    "\n",
    "function se_pred(X, σ̂², xₚ)\n",
    "    return √(σ̂² * (1 + xₚ' * (X'X) * xₚ))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotregion = filter(x-> Dates.month(x) in [6,7,8,9] && Dates.year(x) == 2016,\n",
    "    bikedaily.Date)\n",
    "\n",
    "df_subset = filter(row -> row.Date in plotregion, bikedaily)\n",
    "\n",
    "\n",
    "dailyfit_subset = lm(@formula(cnt ~ thi + tlo + humhi + humlo + wind_speed + weather_code), df_subset)\n",
    "\n",
    "# predicted values in plotregion\n",
    "\n",
    "# Need to get the observations xₚ from df_subset. Problem is xₚ needs our dummy encodings.\n",
    "# yeah, not too happy with my solution for this but whatever\n",
    "\n",
    "covar_subset = dailyfit_subset.mm.m\n",
    "#ha, this is sketch. Doing it cuz this subset doesn't contain all the weather codes I need\n",
    "covar_subset = hcat(covar_subset, zeros(size(covar_subset[1])))\n",
    "se_predictions = [se_pred(X,S², xp) for xp in eachrow(covar_subset)]\n",
    "\n",
    "predicted = predict(dailyfit, df_subset)\n",
    "\n",
    "plot(plotregion, df_subset.cnt, lw=2, label=\"Historical\")\n",
    "plot!(plotregion, predicted, lw=2, label=\"Model\")\n",
    "plot!(plotregion, )"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
